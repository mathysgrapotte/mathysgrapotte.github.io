---
layout: post
title: Stimulus intro
date: 2025-03-07
category: stimulus
slug: stimulus-intro
---

We became explorers of the hyper-parameter space. 

At first glance, hyper-parameters teached about the nature of the model; how does learning rate impact convergence speed? Should models be deeper ? 

The further one wander in this space, the more one realizes that hyper-parmaeters questions our assumptions about the data. If my deeper model learns better, what does it say about the complexity of the data? 

This realization flurished into questions: 
- How do hyper-parameters evolve when data changes?
- Does this help us understand the nature of the data?
- ...
- What if, the goal all along was the hyper-parameters themselves? Equations teach us about the world; more so than their outputs.

One thing is clear to me; a model in a vacuum, without its training data, is incomplete. Like an answer to an unknown question or the output of an hidden equation. 

For this reason, we have built stimulus: a way to fuse data processing and model training in a single, unified, pipeline. 


