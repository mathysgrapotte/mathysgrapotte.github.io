<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.10.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-03-07T18:21:50+01:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">I like bio DL!</title><subtitle>A (nearly) no-CSS, fast, minimalist Jekyll theme.
</subtitle><author><name>mathys grapotte</name></author><entry><title type="html">Unifying Data and Models</title><link href="http://localhost:4000/stimulus-intro.html" rel="alternate" type="text/html" title="Unifying Data and Models" /><published>2025-03-07T00:00:00+01:00</published><updated>2025-03-07T00:00:00+01:00</updated><id>http://localhost:4000/stimulus-intro</id><content type="html" xml:base="http://localhost:4000/stimulus-intro.html"><![CDATA[<p>We became explorers of the hyper-parameter space.</p>

<p>At first glance, hyper-parameters teached about the nature of the model; how does learning rate impact convergence speed? Should models be deeper ?</p>

<p>The further one wander in this space, the more one realizes that hyper-parmaeters questions our assumptions about the data. If my deeper model learns better, what does it say about the complexity of the data?</p>

<p>This realization flurished into questions:</p>
<ul>
  <li>How do hyper-parameters evolve when data changes?</li>
  <li>Does this help us understand the nature of the data?</li>
  <li>…</li>
  <li>What if, the goal all along was the hyper-parameters themselves? Equations teach us about the world; more so than their outputs.</li>
</ul>

<p>One thing is clear to me; a model in a vacuum, without its training data, is incomplete. Like an answer to an unknown question or the output of an hidden equation.</p>

<p>For this reason, we have built stimulus: a way to fuse data processing and model training in a single, unified, pipeline.</p>

<h3 id="data-processing-hyper-space">Data processing hyper-space</h3>

<p>Data processing define their own hyper-parameter space. Should images be rotated? how should the text be tokenized?</p>

<p>As data distances itself from our understanding of the world, processing parametrisation increases in complexity. Which mapper should I use for my sequencing data? Which quality filter? how should I do peak calling? What would my background signal be?</p>

<p>We found that most data processing, regardless of complexity, fall into three atomic steps:</p>
<ol>
  <li>split</li>
  <li>transform</li>
  <li>encode</li>
</ol>

<p>Split defines which subset of the data trains, and which subset of the data validates the model performance. If splitting is done right, it should carefully assess the ability of the model to generalize to unseen examples. I would even argue, that splitting, should let us understand <strong>which unseen examples</strong> the model is able to generalize to.</p>

<p>Transformations define how the data is modified in its raw form. Peak-callers, data cleaning, down/up-sampling, etc. fall into this category. At this stage, the data is still human readable.</p>

<p>Encode describes the final step. At this point, data converts into gpu-readable language. Most underestimate this step, although at this stage, data often undergoes the most drastic changes.</p>

<p>For this reason, stimulus provides a way to define an exploration space for each of these steps. You will find mentions of splitters, encoders and transformers littered accross the documentation and the tool, it is essential that those basic building blocks are configurable and explorable.</p>

<p>Ultimately; model parameters, processing parameters, and performance analytics paint a whole picture describing the data, it is a question alongside its answer, provided you know which question to ask.</p>]]></content><author><name>mathys grapotte</name></author><category term="stimulus" /><summary type="html"><![CDATA[We became explorers of the hyper-parameter space.]]></summary></entry><entry><title type="html">Stimulus refactor</title><link href="http://localhost:4000/stimulus-refactor.html" rel="alternate" type="text/html" title="Stimulus refactor" /><published>2025-02-24T00:00:00+01:00</published><updated>2025-02-24T00:00:00+01:00</updated><id>http://localhost:4000/stimulus-refactor</id><content type="html" xml:base="http://localhost:4000/stimulus-refactor.html"><![CDATA[<h2 id="context">Context</h2>

<p>stimulus separates data processing in three components:</p>
<ul>
  <li>split (into train/validation/test)</li>
  <li>transform (modifications to raw data - downsampling for example)</li>
  <li>encode (raw data to pytorch tensors)</li>
</ul>

<p>Some key observations considering the input data (example adapted from <a href="https://www.kaggle.com/datasets/yasserh/titanic-dataset" target="_blank">kaggle</a>):</p>

<table>
  <thead>
    <tr>
      <th>passenger_id</th>
      <th>survived</th>
      <th>pclass</th>
      <th>sex</th>
      <th>age</th>
      <th>sibsp</th>
      <th>parch</th>
      <th>fare</th>
      <th>embarked</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>0</td>
      <td>3</td>
      <td>male</td>
      <td>22.0</td>
      <td>1</td>
      <td>0</td>
      <td>7.25</td>
      <td>S</td>
    </tr>
    <tr>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>female</td>
      <td>38.0</td>
      <td>1</td>
      <td>0</td>
      <td>71.2833</td>
      <td>C</td>
    </tr>
    <tr>
      <td>3</td>
      <td>1</td>
      <td>3</td>
      <td>female</td>
      <td>26.0</td>
      <td>0</td>
      <td>0</td>
      <td>7.925</td>
      <td>S</td>
    </tr>
    <tr>
      <td>4</td>
      <td>1</td>
      <td>1</td>
      <td>female</td>
      <td>35.0</td>
      <td>1</td>
      <td>0</td>
      <td>53.1</td>
      <td>S</td>
    </tr>
  </tbody>
</table>

<ul>
  <li>Split needs to be called on a specific (group of) column(s).</li>
  <li>Column types vary, thus transforms are column-specific.</li>
  <li>Likewise for encoding.</li>
  <li>Columns serve different purposes:
    <ul>
      <li>input (e.g., <code class="language-plaintext highlighter-rouge">age</code>)</li>
      <li>target (<code class="language-plaintext highlighter-rouge">survived</code>)</li>
      <li>meta-data (<code class="language-plaintext highlighter-rouge">passenger_id</code>)</li>
    </ul>
  </li>
</ul>

<p>Since stimulus is thought to be a command line tool first, these bits and pieces are defined in an external .yaml configuration file. 
For encoders, it looks like this:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">columns</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">column_name</span><span class="pi">:</span> <span class="s">fare</span> 
    <span class="na">column_type</span><span class="pi">:</span> <span class="s">input</span>
    <span class="na">data_type</span><span class="pi">:</span> <span class="s">float</span>
    <span class="na">encoder</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">NumericEncoder</span>
        <span class="na">params</span><span class="pi">:</span>
</code></pre></div></div>

<p>Here, we are defining that <code class="language-plaintext highlighter-rouge">fare</code> is an input column, and need to be encoded using <a href="https://mathysgrapotte.github.io/stimulus-py/reference/stimulus/data/encoding/encoders/#stimulus.data.encoding.encoders.NumericEncoder" target="_blank">NumericEncoder</a></p>

<p>We do the same for transforms:</p>
<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">transforms</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">transformation_name</span><span class="pi">:</span> <span class="s">noise</span>
    <span class="na">columns</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">column_name</span><span class="pi">:</span> <span class="s">age</span>
        <span class="na">transformations</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">GaussianNoise</span>
            <span class="na">params</span><span class="pi">:</span>
              <span class="na">std</span><span class="pi">:</span> <span class="pi">[</span><span class="nv">0.1</span><span class="pi">,</span> <span class="nv">0.2</span><span class="pi">,</span> <span class="nv">0.3</span><span class="pi">]</span>
      <span class="pi">-</span> <span class="na">column_name</span><span class="pi">:</span> <span class="s">fare</span>
        <span class="na">transformations</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">GaussianNoise</span>
            <span class="na">params</span><span class="pi">:</span>
              <span class="na">std</span><span class="pi">:</span> <span class="pi">[</span><span class="nv">0.1</span><span class="pi">,</span> <span class="nv">0.2</span><span class="pi">,</span> <span class="nv">0.3</span><span class="pi">]</span>
</code></pre></div></div>

<p>Here we apply <a href="https://mathysgrapotte.github.io/stimulus-py/reference/stimulus/data/transform/data_transformation_generators/#stimulus.data.transform.data_transformation_generators.GaussianNoise" target="_blank">GaussianNoise</a> to two different columns with three different standard deviation parameters.</p>

<p>and we define the split parameters as such :</p>
<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">split</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">split_method</span><span class="pi">:</span> <span class="s">RandomSplit</span>
    <span class="na">split_input_columns</span><span class="pi">:</span> <span class="pi">[</span><span class="nv">age</span><span class="pi">]</span>
    <span class="na">params</span><span class="pi">:</span>
      <span class="na">split</span><span class="pi">:</span> <span class="pi">[</span><span class="nv">0.7</span><span class="pi">,</span> <span class="nv">0.15</span><span class="pi">,</span> <span class="nv">0.15</span><span class="pi">]</span>
</code></pre></div></div>

<p>We use a <a href="https://mathysgrapotte.github.io/stimulus-py/reference/stimulus/data/splitters/#stimulus.data.splitters.RandomSplit" target="_blank">RandomSplit</a> splitter on the <code class="language-plaintext highlighter-rouge">age</code> column, separating the data into 70% for training, 15% for validation and 15% for testing.</p>

<h2 id="interfacing-configuration-files-with-the-data-the-old-way">Interfacing configuration files with the data, the old way</h2>

<p>The configuration file serves as an interface between the data and the code, for this to happen correctly, we need to:</p>

<ul>
  <li>General
    <ol>
      <li>understand which columns are input, meta-data or target(label).</li>
    </ol>
  </li>
  <li>Encoders
    <ol>
      <li>link the encoder with its column.</li>
      <li>fetch the proper encoder from the codebase.</li>
      <li>use the right encoder on the right column when considering the full dataset, taking care of input, meta-data or target considerations.</li>
    </ol>
  </li>
  <li>Transforms
    <ol>
      <li>link transforms with their columns.</li>
      <li>fetch the proper transforms from the codebase.</li>
      <li>remember the order of transforms within a group (e.g. the <code class="language-plaintext highlighter-rouge">noise</code> group defined above).</li>
      <li>use the right transform group on the right columns when considering the full dataset.</li>
    </ol>
  </li>
  <li>Split
    <ol>
      <li>fetch the proper splitter from the codebase.</li>
      <li>use it on the right column or set of columns.</li>
    </ol>
  </li>
</ul>

<p>Until now, we dealt with the problem by providing three levels of abstractions.</p>

<ol>
  <li>Loaders, <a href="https://mathysgrapotte.github.io/stimulus-py/reference/stimulus/data/loaders/#stimulus.data.loaders.EncoderLoader" target="_blank">for example, EncoderLoader</a> includes boilerplate code to load from the configuration file, fetch the proper encoder from the codebase and get its <code class="language-plaintext highlighter-rouge">encode_all</code> method.</li>
  <li>Managers, <a href="https://mathysgrapotte.github.io/stimulus-py/reference/stimulus/data/data_handlers/#stimulus.data.data_handlers.EncodeManager">for example, EncodeManager</a>, holds the logic of which encoder to use on which column and boilerplate code to apply encoders to a subset of the dataframe.</li>
  <li>Finally, the <a href="https://mathysgrapotte.github.io/stimulus-py/reference/stimulus/data/data_handlers/#stimulus.data.data_handlers.DatasetHandler">dataset handler</a> main class, split into <code class="language-plaintext highlighter-rouge">DatasetLoader</code> and <code class="language-plaintext highlighter-rouge">DatasetProcessor</code>, contains boilerplate code to load the data from disk, apply transformations and split, and feed the data to the network as a dictionary of PyTorch tensors.</li>
</ol>

<p>If you would need to visualize it, would look like this:</p>

<div style="width: 60vw; margin-left: calc(-27vw + 50%); margin-right: calc(-25vw + 50%); text-align: center;">
<img src="https://mermaid.ink/svg/pako:eNptlOFrnDAYxv8VSb-mrF57PerKYMyDGxg2UAab9sM7E0-pJpLoh9Hr_75oEk-twoF53-eX5PHJ5Q3lgjIUoLOEtvSSMOOeflT_1xQiAZRJZarDc4zSIx8YaVov3u3tl0vBGFUX70iuwiRKEwlcFUI2G9JkJo2jNG7rqtuQxVbGOM34am8EOJwXmwtJGkIHinW2Z-bKBS-q88ULTzMfxPpYKNlY0uuGZi8zO-RqZ4F0rjpQP6XImVJiBsbEmFtAaqhsAFs2v3_6MXN4cg5PwGnt5qt4yeTmjAZai9burpSbfiq8PGvWu7RmPH4brZihZi7HbWRI9CetP_j7NoaS_v5KIvs-QjpD249_pfo3rmY7Nr6Q2FTrvuGeC9edPdudYpkEyVIwRjA1h3Nm2nkNSoWs8OrRiVdUdR3c-LAvDoBVJ8UrC27uYJff088rojERW-TpYf_gP03Io3__6N-tkdKkaJEd3cPuMCE-aOQwR_TfDycRjiO7u0WL4ITgmGBt0O5k3g5P-Joytqm59RFGDZMNVFRfBW8DlqGuZA3LUKBfKcjXDGX8Xeug70T8j-co6GTPMJKiP5du0LcUOhZWoI9u44ot8D9C6GEBtdJjRqtOSGLunfH6ef8P-8t0hA" />
</div>

<p>This outlines two main problems:</p>
<ol>
  <li>Three levels of abstraction, the codebase is too hard to understand.</li>
  <li>Tight coupling between the various classes, if you change one module, everything breaks.</li>
</ol>

<p>point 1. is fairly easy to understand by looking at the above diagram, so let’s discuss point 2.</p>

<p>point 2. is best understood by looking at the current <a href="https://mathysgrapotte.github.io/stimulus-py/reference/stimulus/data/handlertorch/#stimulus.data.handlertorch.TorchDataset" target="_blank">TorchDataset class</a> which interfaces the data with the neural network.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">TorchDataset</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="s">"""Class for creating a torch dataset."""</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">config_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">csv_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">encoder_loader</span><span class="p">:</span> <span class="n">loaders</span><span class="p">.</span><span class="n">EncoderLoader</span><span class="p">,</span>
        <span class="n">split</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="s">"""Initialize the TorchDataset.

        Args:
            config_path: Path to the configuration file
            csv_path: Path to the CSV data file
            encoder_loader: Encoder loader instance
            split: Optional tuple containing split information
        """</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">loader</span> <span class="o">=</span> <span class="n">data_handlers</span><span class="p">.</span><span class="n">DatasetLoader</span><span class="p">(</span>
            <span class="n">config_path</span><span class="o">=</span><span class="n">config_path</span><span class="p">,</span>
            <span class="n">csv_path</span><span class="o">=</span><span class="n">csv_path</span><span class="p">,</span>
            <span class="n">encoder_loader</span><span class="o">=</span><span class="n">encoder_loader</span><span class="p">,</span>
            <span class="n">split</span><span class="o">=</span><span class="n">split</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">loader</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">dict</span><span class="p">,</span> <span class="nb">dict</span><span class="p">,</span> <span class="nb">dict</span><span class="p">]:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">loader</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
</code></pre></div></div>
<p>This function, which should be simple in its implementation, will perform lots of things under the hood:</p>
<ul>
  <li>load the data from disk using the proper split (train/validation/test) if needed</li>
  <li>load the configuration file defined above</li>
  <li>make sure encoder loader fits</li>
  <li>return the data in a format that can be used by the neural network</li>
</ul>

<p>If any of those steps is changed somewhere else in the code (for instance, <code class="language-plaintext highlighter-rouge">DatasetLoader</code> config path argument is renamed), TorchDataset will break. Every little change requires hours of debugging.</p>

<p>Since scientific code requires to be extremely flexible (to try the new cool things), we want to minimize the time it takes to implement a change in the codebase.</p>

<h2 id="interfacing-configuration-files-with-the-data-the-new-way">Interfacing configuration files with the data, the new way</h2>

<p>The first idea here is to remove as many abstractions as possible. <code class="language-plaintext highlighter-rouge">Encoders</code>, <code class="language-plaintext highlighter-rouge">Transforms</code>, and <code class="language-plaintext highlighter-rouge">Splitters</code> are non-compressible core functionalities, same goes with <code class="language-plaintext highlighter-rouge">DatasetHandler</code> classes.</p>

<p>Focusing on native python data structures (such as dictionaries, lists, etc.) will make the codebase readable (our contributors understand dictionaries, not necessarly <code class="language-plaintext highlighter-rouge">DatasetManagers</code>).</p>

<p>Think about it in this way: the more concepts contributors need to learn, the more likely they will quit before the first PR.</p>

<p>For this, we need to rethink the way we do I/O management in between our elements. Config parsing has to be outsourced to a dedicated module, which shall output simple, native python objects. <code class="language-plaintext highlighter-rouge">DatasetHandler</code> classes will expect those objects as input, and will not do the parsing themselves (one class should do one thing):</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DatasetLoader</span><span class="p">(</span><span class="n">DatasetHandler</span><span class="p">):</span>
    <span class="s">"""Class for loading dataset and passing it to the deep learning model."""</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">encoders</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">encoders_module</span><span class="p">.</span><span class="n">AbstractEncoder</span><span class="p">],</span>
        <span class="n">input_columns</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
        <span class="n">label_columns</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
        <span class="n">meta_columns</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
        <span class="n">csv_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">split</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
    <span class="p">...</span>
</code></pre></div></div>

<p>Here, instead of having loaders and managers, we load needed information directly to the <code class="language-plaintext highlighter-rouge">DatasetHandler</code> class, in simple, native python objects. For example, the class itself is not expected to find out which columns are input, labels or meta-data; this was done beforehand.</p>

<p>Notice as well, that the encoders are expected in a simple dictionary of format <code class="language-plaintext highlighter-rouge">{column_name: encoder_instance}</code>. When needing to find an encoder for a specific column, we only need to access the dictionary with the name of the column that we want to encode as the key.</p>

<p>This allows us to completely decouple the <code class="language-plaintext highlighter-rouge">DatasetHandler</code> class from the configuration file parsing. If the configuration file format changes, DatasetHandler does not care about it (as it always expects objects to be already parsed), which addresses point 2!</p>

<p>To further explain how we decouple the codebase, lets rewrite the <code class="language-plaintext highlighter-rouge">TorchDataset</code> class:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">TorchDataset</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="s">"""Class for creating a torch dataset."""</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">loader</span><span class="p">:</span> <span class="n">DatasetLoader</span><span class="p">,</span> 
        <span class="c1"># loader is initialized outside of TorchDataset
</span>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="s">"""Initialize the TorchDataset.

        Args:
            loader: A DatasetLoader instance
        """</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">loader</span> <span class="o">=</span> <span class="n">loader</span>

    <span class="k">def</span> <span class="nf">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">loader</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">dict</span><span class="p">,</span> <span class="nb">dict</span><span class="p">,</span> <span class="nb">dict</span><span class="p">]:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">loader</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
</code></pre></div></div>

<p>Here, as long as <code class="language-plaintext highlighter-rouge">DatasetLoader</code> implements the <code class="language-plaintext highlighter-rouge">__getitem__</code> and <code class="language-plaintext highlighter-rouge">__len__</code> methods, TorchDataset will work. Changing the inner working of <code class="language-plaintext highlighter-rouge">DatasetLoader</code> will not affect <code class="language-plaintext highlighter-rouge">TorchDataset</code>!</p>

<p>Altogether, those changes make the codebase intuitive, and the data flows streamlined, if we rebuild the class diagram, it would look like this:</p>

<div style="width: 40vw; margin-left: calc(-20vw + 50%); margin-right: calc(-25vw + 50%); text-align: center;">
<img src="https://mermaid.ink/svg/pako:eNp1kk2PwiAQhv8KwWtNrJ-xu9mL3cSLiUmNh7V7mC1gGykQoIeN-t-Xlqq1GznBzPPOF3PGmSQUR_ioQeVoF6cCuWOqH29YScGK4xa0odq76rPaHrwDec83Gg4_Lqq-mwv6FHVMbbr8E7DTIAyTunyNJIoX9h6DCpKKXmUxWDDUrkEQ3q0tXh-eXb646kVp8bpxg3IJX9XWZ56La4n3GuESiANWyf6fmlFauzYuP__XVcbBmJgy1PSvESs4jwYhzNgCAmO1PNFoMIJxNiFvPUXuu2wlYzKD8eIuCWEyDxd9CXHzafnldDYNl3d-Hjp-1OXrj1Gd7_dG11PeHXyLJvvgNuHgMcjgNq8mMQ5wSXUJBXFbd67lKbY5LWmKI3cloE8pTsXVcVBZmfyKDEdWVzTAWlbH_PaolItG4wLcOpQ4YsCNsyoQX1I-3pQUVuqN3_Fm1a9_Io_x4Q" />
</div>

<p>Way better right ?</p>

<p>You can follow the refactoring effort on the <a href="https://github.com/users/mathysgrapotte/projects/2" target="_blank">project board</a>.</p>]]></content><author><name>mathys grapotte</name></author><category term="stimulus" /><summary type="html"><![CDATA[Context]]></summary></entry><entry><title type="html">How to build open science software?</title><link href="http://localhost:4000/howto-open-science-software.html" rel="alternate" type="text/html" title="How to build open science software?" /><published>2025-02-21T00:00:00+01:00</published><updated>2025-02-21T00:00:00+01:00</updated><id>http://localhost:4000/howto%20open%20science%20software</id><content type="html" xml:base="http://localhost:4000/howto-open-science-software.html"><![CDATA[<p>I like to categorise scientific software in levels:</p>
<ul>
  <li>analysis scripts and notebooks</li>
  <li>packages</li>
  <li>open science software</li>
</ul>

<p>open science software needs to be</p>]]></content><author><name>mathys grapotte</name></author><category term="open-science" /><summary type="html"><![CDATA[I like to categorise scientific software in levels: analysis scripts and notebooks packages open science software]]></summary></entry></feed>